Venue,Review ID,review,Tone,Review URL
ICLR,B11bwYgfM_R1,Please refer to the paper Discovering structure in multiple learning tasks : The TC algorithm published in ICML 1996 .,5,
ICLR,B11bwYgfM_R1,"One issue of the use of cross task transfer performance to measure task relations is that it ignores the negative correlations between tasks , which is useful for learning from multiple tasks .",3,
ICLR,B11bwYgfM_R2,"Learning incoherent sparse and low rank patterns from multiple tasks ( KDD ) In particular , [ 5 ] assumes that the combined weight matrix ( for all the tasks ) follows the robust PCA model .",3,
ICLR,B11bwYgfM_R2,"However , a disadvantage of the proposed method is that it is a two step approach ( first perform task clustering , then re learn the cluster weights ) , while [ 5 ] is not .",3,
ICLR,B11bwYgfM_R2,- Comparison with existing clustered MTL methods mentioned above are missing .,3,
ICLR,B11bwYgfM_R2,"- As mentioned above , the proposed method can be computationally expensive ( when used for MTL ) , but no timing results are reported .",3,
ICLR,B11bwYgfM_R3,"Prior techniques which can address some of these aspects do not necessarily work with deep learning , which is a key focus of the paper .",3,
ICLR,B11bwYgfM_R3,"I think there are some interesting ideas in this paper , and the use of matrix completion techniques to deal with a large number of tasks is nice .",4,
ICLR,B11bwYgfM_R3,But I believe there are important drawbacks in the framing and basic methodology and evaluation which make the paper unfit for publication in its current form .,3,
ICLR,B11bwYgfM_R3,The pairwise similarity measure appears to be one that might have a high false negative rate .,3,
ICLR,B11bwYgfM_R3,I do not see how you apply the model from task i to task j when the two have different output spaces .,2,
ICLR,B11bwYgfM_R3,"Since this is a major motivation of the paper , I actually do not see how the setup makes sense !",1,
ICLR,B11bwYgfM_R3,The inducing of edges in the Y matrix by comparing to a mean and standard deviation is completely baseless .,2,
ICLR,B11bwYgfM_R3,"Without good reasoning from the authors , I see no reason why the entries in the row of a matrix should have a normal like distribution .",2,
ICLR,B11bwYgfM_R3,"In this case , the means are standard deviations do not even make sense to me .",2,
ICLR,B11bwYgfM_R3,"At the very least , I would consider using regret to the model of the task , and compute some quantiles on that which is still suspect in the matrix completion setting .",3,
ICLR,B11bwYgfM_R3,"If the error rates are different for different tasks , it is not sensible to measure raw accuracies .",2,
ICLR,B11bwYgfM_R3,"The authors also seem to miss a potentially relevant baseline in Cross Stitch Networks. Besides these major issues , there are also a few minor issues I have with the paper .",3,
ICLR,B11bwYgfM_R3,"Given this , the presentation in the paper makes the idea look more novel than it is .",3,
ICLR,B11bwYgfM_R3,"I also think that the authors might benefit from dropping the whole few shot learning angle here , and instead do a more thorough job of evaluating their multitask learning method .""",4,
ICLR,B12Js_yRb_R1,Contribution : - This paper proposes a new object counting module which operates on a graph of object proposals .,3,
ICLR,B12Js_yRb_R1,Cons : - The proposed model is pretty hand crafted .,2,
ICLR,B12Js_yRb_R1,"I would recommend the authors to use something more general , like graph convolutional neural networks or graph gated neural networks.",4,
ICLR,B12Js_yRb_R1,- One major bottleneck of the model is that the proposals are not jointly finetuned .,3,
ICLR,B12Js_yRb_R1,The paper didnt study what is the recall of the proposals and how sensitive the threshold is .,4,
ICLR,B12Js_yRb_R1,- The paper doesnt study a simple baseline that just does NMS on the proposal domain .,4,
ICLR,B12Js_yRb_R1,This is similar to a density map approach and the problem is that the model doesnt develop a notion of instance .,4,
ICLR,B12Js_yRb_R1,"- Since the authors have mentioned in the related work , it would also be more convincing if they show experimental results on CL Conclusion : - I feel that the motivation is good , but the proposed model is too hand crafted .",4,
ICLR,B12Js_yRb_R1,Therefore I recommend reject .,2,
ICLR,B12Js_yRb_R1,"The rebuttal is convincing and I decided to increase my rating , because adding the proposed counting module achieve 5 % increase in counting accuracy .",4,
ICLR,B12Js_yRb_R2,""" Summary - This paper mainly focuses on a counting problem in visual question answering ( VQA ) using attention mechanism .",3,
ICLR,B12Js_yRb_R2,Strengths - The proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly .,5,
ICLR,B12Js_yRb_R2,"Weaknesses - Although the proposed model is helpful to model counting information in VQA , it fails to show improvement with respect to a couple of important baselines : prediction from image representation only and from the combination of image representation and attention weights .",4,
ICLR,B12Js_yRb_R2,"It is not clear if the value of count "" c "" is same with the final answer in counting questions .",3,
ICLR,B12Js_yRb_R3,"this method is built on a series of heuristics without sound theoretically justification , and these heuristics cannot be easily adapted to other machine learning applications .",3,
ICLR,B12Js_yRb_R3,I believe the overall contribution is not sufficient for ICLR .,2,
ICLR,B12Js_yRb_R3,Well written paper with clear presentation of the method .,4,
ICLR,B12Js_yRb_R3,Can the author provide analysis on scalability the proposed method ?,3,
ICLR,B13EC5u6W_R1,This is quite an interesting paper with a sensible goal. It seems like the method could be more informative than the other methods .,4,
ICLR,B13EC5u6W_R1,"It would be v interesting ( and provide a good baseline ) to use a shallow network ( i.e . PCA ) instead , and elucidate what advantages the deep network brings .",4,
ICLR,B13EC5u6W_R1,The reconstructions show poor detail relative to the originals .,3,
ICLR,B13EC5u6W_R1,"* Overall : the paper contains an interesting idea , but given the deficiencies raised above I judge that it falls below the ICLR threshold .",4,
ICLR,B13EC5u6W_R2,The proposed method uses - a generator trained in a GAN setup - an autoencoder to obtain a latent space representation - a method inspired by adversarial sample generation to obtain a generated image from another class.,3,
ICLR,B13EC5u6W_R2,It would be great if the authors could come up with an additional way to demonstrate their method to the non medical reader .,4,
ICLR,B13EC5u6W_R2,The authors train their system with the testdata included which leads to very different visualizations .,4,
ICLR,B13EC5u6W_R2,This looks very noisy and non interesting .,2,
ICLR,B13EC5u6W_R3,This focus is mainly on medical image classification but the approach could potentially be useful in many more areas .,4,
ICLR,B13EC5u6W_R3,This gives an image that is similar to the original but with features that caused the classification of the disease removed .,3,
ICLR,B13EC5u6W_R3,The resulting image can be subtracted from the original encoding to highlight problematic areas .,3,
ICLR,B13EC5u6W_R3,"The idea in the paper is , to my knowledge , novel , and represents a good step toward the important task of generating interpretable visual rationales .",4,
ICLR,B13EC5u6W_R3,"There are a few details missing , like the batch sizes used for training ( it is difficult to relate epochs to iterations without this ) .",3,
ICLR,B13EC5u6W_R3,"It is also a little confusing that you begin this paragraph saying that you are doing a classification task , but then it seems like a regression task which may be postprocessed to give a classification .",3,
ICLR,B13njo1R-_R1,"Section 4.1 states""We use a method similar to the DAGGER algorithm "" , but what is your method .",2,
ICLR,B13njo1R-_R1,"- I do not understand the purpose of "" input injection "" nor where it is used in the paper .",2,
ICLR,B13njo1R-_R1,The method is simple but novel,3,
ICLR,B13njo1R-_R1,Figure 2 : the plots are too small .,3,
ICLR,B13njo1R-_R1,The paper cites many previous approaches to this but does not compare against any of them .,3,
ICLR,B13njo1R-_R1,- A second testbed ( such as navigation or manipulation ) would bring the paper up a notch .,4,
ICLR,B13njo1R-_R1,The method is clear but not precisely described .,3,
ICLR,B13njo1R-_R2,"PLAID masters several distinct tasks in sequence , building up skills by learning related tasks of increasing difficulty .",3,
ICLR,B13njo1R-_R2,"- Although the main focus of this paper is on continual learning of related tasks , the authors acknowledge this limitation and convincingly argue for the chosen task domain .",4,
ICLR,B13njo1R-_R3,But I find the paper lacking a lot of details and to some extend confusing .,2,
ICLR,B13njo1R-_R3,Please first of all make the figures much larger .,5,
ICLR,B13njo1R-_R3,"ICLR does not have a strict page limit , and the figures you have are hard to impossible to read .",3,
ICLR,B13njo1R-_R3,"Indeed , because the retention of tasks is done by distilling all of them jointly , one baseline is to keep finetuning a model through the 5 stages , and then at the end after collecting the 5 policies you can do a single consolidation step that compresses all .",3,
ICLR,B13njo1R-_R3,Please say in the main text that details in terms of architecture and so on are given in the appendix .,5,
ICLR,B13njo1R-_R3,"But I want to make sure the authors put a bit more effort into cleaning up the paper , making it more clear and easy to read .",3,
ICLR,B14TlG-RW_R1,The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference .,3,
ICLR,B14TlG-RW_R1,The proposed model is convincing and the paper is well written .,4,
ICLR,B14TlG-RW_R1,The proposed data augmentation is a general one and it can be used to improve the performance of other models as well .,3,
ICLR,B14TlG-RW_R1,I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models .,4,
ICLR,B14TlG-RW_R1,3 . The paper can be significantly strengthened by adding at least one more reading comprehension dataset .,4,
ICLR,B14TlG-RW_R1,"Given the sufficient time for rebuttal , I am willing to increase my score if authors report results in an additional dataset in the revision .",4,
ICLR,B14TlG-RW_R1,Please be consistent .,4,
ICLR,B14TlG-RW_R2,"Firstly , I suggest the authors rewrite the end of the introduction .",3,
ICLR,B14TlG-RW_R2,The current version tends to mix everything together and makes the misleading claim .,3,
ICLR,B14TlG-RW_R2,The novelty is limited but it is a good idea to speed up the RC models .,4,
ICLR,B14TlG-RW_R2,"However , as the authors hoped to claim that this module could contribute to both speedup and RC performance , it will be necessary to show the RC performance of the same model architecture , but replacing the CNNs with LSTMs .",3,
ICLR,B14TlG-RW_R2,I feel that the model design is the main reason for the good overall RC performance .,4,
ICLR,B14TlG-RW_R2,"However , in the paper there is no motivation about why the architecture was designed like this .",3,
ICLR,B14TlG-RW_R2,it is not convincing that the system design has good generalization .,3,
ICLR,B14TlG-RW_R2,I like the idea of data augmentation with paraphrasing .,4,
ICLR,B14TlG-RW_R3,"At the moment , the description in section 3 is fuzzy in my opinion .",2,
ICLR,B14uJzW0b_R1,Importance : Understanding the landscape ( local vs global minima vs saddle points ) is an important direction in order to further understand when and why deep neural networks work .,3,
ICLR,B14uJzW0b_R1,"To the best of my understanding , the paper has some misconceptions .",3,
ICLR,B14uJzW0b_R1,"Another issue is the fact that , on my humble opinion , the main text looks like a long proof .",3,
ICLR,B14uJzW0b_R1,"The paper mainly focuses on a specific problem instance , where the weight vectors are unit normed and orthogonal to each other .",3,
ICLR,B14uJzW0b_R1,"The paper reads like a collection of lemmas , with no verbose connection .",3,
ICLR,B14uJzW0b_R1,"It was hard to read and understand their value , just because mostly the text was structured as one lemma after the other .",3,
ICLR,B14uJzW0b_R2,""" In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem , whose regressor is a simple neural network .",3,
ICLR,B14uJzW0b_R2,"I also appreciate the effort of developing theoretical results for deep learning , even though the current results are restrictive to very simple NN architectures .",4,
ICLR,B14uJzW0b_R2,"The analysis becomes considerably more complicated , and the contribution seems to be novel and significant .",4,
ICLR,B14uJzW0b_R2,I am not sure why did the authors mentioned the work on over parameterization though .,3,
ICLR,B14uJzW0b_R2,"While the Re LU activation is very common in NN architecture , without more motivations I am not sure what are the impacts of these results .",2,
ICLR,B14uJzW0b_R2,"While I appreciate the technical contributions , in order to improve the readability of this paper , it would be great to see more motivations of the problem studied in this paper ( even with simple examples ) .",3,
ICLR,B14uJzW0b_R3,""" This paper considers a special deep learning model and shows that in expectation , there is only one unique local minimizer .",3,
ICLR,B16_iGWCW_R1,Its not clear what kind of loss function is really being optimised here .,2,
ICLR,B16_iGWCW_R1,"It feels like it should be the same , but the tweaks applied to fix weights across all samples for a class doesnt make it not clear what is that really gets optimised at the end .",3,
ICLR,B16_iGWCW_R1,"And crudely speaking , you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup .",3,
ICLR,B16_iGWCW_R1,"- Experimentally , paper would benefit with better comparisons and studies : 1 ) state of the art methods havent been compared against ( e.g . Image Net experiment compares to 2 years old method ) 2 ) comparisons to using normal Ada Boost on more complex methods havent been studied ( other than the MNIST ) 3 ) comparison to simply ensembling with random initialisations .",4,
ICLR,B16_iGWCW_R1,Paper would benefit from writing improvements to make it read better .,3,
ICLR,B16_iGWCW_R2,The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right .,3,
ICLR,B16_iGWCW_R2,Thus the learned weak learner at this round will make different mistakes .,3,
ICLR,B16_iGWCW_R2,This paper instead designed a new boosting method which puts large weights on the category with large error in this round .,4,
ICLR,B16_iGWCW_R2,"Experiments show its usefulness though experiments are limited """,3,
ICLR,B16_iGWCW_R3,There is hardly any baseline compared in the paper .,3,
ICLR,B16yEqkCZ_R1,""" The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states .",3,
ICLR,B16yEqkCZ_R1,The authors propose to train a predictive fear model that penalizes states that lead to catastrophes .,3,
ICLR,B16yEqkCZ_R1,Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally .,3,
ICLR,B16yEqkCZ_R1,No empirical results on the effect of the parameter are given .,3,
ICLR,B16yEqkCZ_R1,This problem does not seem to be really solved by this method .,2,
ICLR,B16yEqkCZ_R1,It would therefore be interesting to see some long running experiments and analyse how often catastrophic states ( or those close to them ) are visited .,4,
ICLR,B16yEqkCZ_R1,"The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ( [ 1,2 ] ) .",3,
ICLR,B16yEqkCZ_R1,The motivation in the appendix is very informal and no clear derivation is provided .,3,
ICLR,B16yEqkCZ_R1,"Any optimal policy would therefore need to spend some time e in the danger state , on average .",3,
ICLR,B16yEqkCZ_R1,It wasnt clear what assumptions the authors make to exclude situations like this .,4,
ICLR,B16yEqkCZ_R2,It seems that catastrophe states need to be experienced at least once .,3,
ICLR,B16yEqkCZ_R3,""" The paper studies catastrophic forgetting , which is an important aspect of deep reinforcement learning ( RL ) .",3,
ICLR,B16yEqkCZ_R3,"I believe this observation , though not entirely novel , will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues .",4,
ICLR,B16yEqkCZ_R3,"The paper is accurate , very well written ( apart from a small number of grammatical mistakes ) and contains appealing motivations to its key contributions .",5,
ICLR,B16yEqkCZ_R3,"In fact , it is not difficult to design examples for which the proposed algorithm would be far from optimal .",3,
ICLR,B16yEqkCZ_R3,A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems .,3,
ICLR,B17JTOe0-_R1,The paper is overall quite interesting and the study is pretty thorough : no major cons come to mind .,4,
ICLR,B17JTOe0-_R1,That seems important to explain more thoroughly than is done in the current text .,3,
ICLR,B17JTOe0-_R2,"While ICLR is not focused on neuroscientific studies , this paper clearly belongs here as it shows what representations develop in recurrent networks that are trained on spatial navigation .",4,
ICLR,B17JTOe0-_R2,The paper mentions a metabolic cost that is not specified in the paper .,3,
ICLR,B17JTOe0-_R2,I am puzzled why is the error is coming down before the boundary interaction ?,2,
ICLR,B17JTOe0-_R3,"The manuscript is not written in a way that is suitable for the target ICLR audience which will include , for the most part , readers that are not expert on the entorhinal cortex and/or spatial navigation .",3,
ICLR,B17JTOe0-_R3,"For instance , in the introduction , it is stated that previous attractor network type of models ( which are also recurrent networks ) [...] require hand crafted and fined tuned connectivity patterns , and the evidence of such specific 2D connectivity patterns has been largely absent .",4,
ICLR,B17JTOe0-_R3,I found the study of the effect of regularization to be potentially the most informative for neuroscience but it is only superficially treated .,2,
ICLR,B18WgG-CZ_R1,"Its not very surprising that adding more tasks and data improves performance on average across downstream tasks , but it is nice to see the experimental results in detail .",4,
ICLR,B18WgG-CZ_R1,I also like how the authors move beyond the standard sentence tasks to evaluate also on the Quora question duplicate task with different amounts of training data and also consider the sentence characteristic / syntactic property tasks .,4,
ICLR,B18WgG-CZ_R1,"Regarding the results in Table 2 : The results in Table 2 seem a little bit unstable , as it is unclear which setting to use for the classification tasks ; maybe it depends on the kind of classification being performed .",3,
ICLR,B18WgG-CZ_R1,"Adding parsing as a training task hurts performance on the sentence classification tasks while helping performance on the semantic tasks , as the authors note .",4,
ICLR,B18WgG-CZ_R1,"Did the authors replicate the results of those methods themselves , or report them from other papers ?",4,
ICLR,B18WgG-CZ_R1,"For other results on these datasets , including stronger results in non fixed dimensional sentence embedding transfer settings , see results and references in Mc Cann et al .",3,
ICLR,B18WgG-CZ_R1,"For all tasks for which there is additional training , theres a confound due to the dimensionality of the sentence embeddings across papers .",3,
ICLR,B18WgG-CZ_R1,It doesnt make much sense to me to omit pairs with OOVs .,2,
ICLR,B18WgG-CZ_R1,There are much better embeddings on Sim Lex than the embeddings whose results are reported in the table .,3,
ICLR,B18WgG-CZ_R1,Some readers may mistakenly think that the embeddings are SOTA on Sim Lex since no stronger results are provided in the table .,3,
ICLR,B18WgG-CZ_R1,I feel the motivation for this is lacking .,2,
ICLR,B18WgG-CZ_R1,I would suggest adding some motivation for the focus on fixed length representations .,3,
ICLR,B18WgG-CZ_R2,"The authors mostly addressed my main concern , which was the relatively weak ablation .",4,
ICLR,B18WgG-CZ_R2,"I disagree with the response , though , that the authors can lean on other papers to help fill in the ablationevery paper in this area uses subtly different configurations .",2,
ICLR,B18WgG-CZ_R2,"I have one small lingering concern , which is not big enough to warrant acceptance : R2s point 10 is validthe use of multiple RNNs trained on different objectives in the ablation experiments unexpected and unusual , and deserves mention in the body of the paper , rather than only in an appendix .",3,
ICLR,B18WgG-CZ_R3,"Another question I have is that if there is any specific reason that language modeling is not included as one of the training objectives to learn sentence representations , given that it seems to be the easiest one to collect training data for .",4,
ICLR,B1CEaMbR-_R1,The claimed main contribution of the paper is the taxonomy .,3,
ICLR,B1CEaMbR-_R1,Therefore the impact or actual contribution to the ICLR community is very limited .,3,
ICLR,B1CEaMbR-_R3,"Furthermore , in Section 4 a new method is proposed , that is to combine the best parts of the already existing models in the literature .",3,
ICLR,B1CEaMbR-_R3,"Also , it overall rather appears short .",3,
ICLR,B1CNpYg0-_R1,"- I would be slightly surprised if no previous work has used external resources for training word representations using an end task loss , but I dont know the area well enough to make specific suggestions - Im a little skeptical about how often this method would really be useful in practice .",2,
ICLR,B1CNpYg0-_R3,"The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas ( character based models , using dictionary definitions ) to implement them as part of a model trained on the end task .",5,
ICLR,B1CNpYg0-_R3,"In general , for the scope of the paper , it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category .",3,
ICLR,B1CNpYg0-_R3,"While this argument has intuitive appeal , it seems to fly in the face of the fact that actually spelling models , including in this paper , seem to do surprisingly well at learning such arbitrary semantics .",4,
ICLR,B1CQGfZ0b_R1,"The proposed approach is interesting , but I feel that the experimental section does not serve to show its merits for several reasons .",3,
ICLR,B1CQGfZ0b_R1,"Given that the results are very close to other approaches , it remains unclear whether they are simply due to random variations , or whether the proposed approach actually achieves a non random improvement .",3,
ICLR,B1CQGfZ0b_R1,"The border pixels are probably sufficient to learn the program perfectly , and in fact this may be exactly what the neural net is learning .",4,
ICLR,B1CQGfZ0b_R2,"The paper isnt presented in exactly these terms , but the idea is to consider a uniform distribution over programs and a zero one likelihood for input output examples ( so observations of I/O examples just eliminate inconsistent programs ) .",3,
ICLR,B1CQGfZ0b_R2,"I cant see how this approach would be tractable in more standard program synthesis domains where inputs might be lists of arrays or strings , for example .",3,
ICLR,B1CQGfZ0b_R2,"Not only is it impossible to reproduce a paper without any architectural details , but the result is then that Fig 3 essentially says inputs -> "" magic "" -> outputs .",2,
ICLR,B1CQGfZ0b_R2,- This paper is poor in the reproducibility category .,2,
ICLR,B1CQGfZ0b_R2,"Overall : Theres the start of an interesting idea here , but I dont think the quality is high enough to warrant publication at this time .",3,
ICLR,B1CQGfZ0b_R3,""" General purpose program synthesizers are powerful but often slow , so work that investigates means to speed them up is very much welcome this paper included .",3,
ICLR,B1CQGfZ0b_R3,"For the most paper , the paper is clearly written , with each design decision justified and rigorously specified .",4,
ICLR,B1CQGfZ0b_R3,The paper did not specify how often the neural net must be trained .,3,
ICLR,B1CQGfZ0b_R3,These are all points that require discussion which is currently missing from the paper .,3,
ICLR,B1CQGfZ0b_R3,"Page 5 : note that we do not suggest a specific neural network architecture for the middle layers , one should select whichever architecture that is appropriate for the domain at hand - such as ?",2,
ICLR,B1D6ty-A-_R1,"I do want to note my other concerns : I suspect the theoretical results obtained here are somewhat restricted to the least squares , autoencoder loss .",2,
ICLR,B1D6ty-A-_R1,A negative answer to this question will somewhat undermine the significance of the single hidden layer result .,3,
ICLR,B1D6ty-A-_R1,"Practically , even if the authors can perform efficient optimization of weights in individual layers , when there are many layers , the alternating optimization nature of the algorithm can possibly result in overall slower convergence .",3,
ICLR,B1D6ty-A-_R2,"Yes they are useful if one was doing 2 layer neural networks for binary classification , but it is not clear to me how they are useful for autoencoder problems .",2,
ICLR,B1D6ty-A-_R2,3 . Experimental results for classification are not convincing enough .,3,
ICLR,B1D6ty-A-_R2,I would recommend the authors to rerun these experiments but truncate the iterations early enough .,3,
ICLR,B1DmUzWAW_R1,Design choices made for the reinforcement learning setup ( e.g . temporal convolutions ) are not necessarily applicable to few shot classification .,3,
ICLR,B1DmUzWAW_R1,Discussion of results relative to baselines is somewhat lacking .,3,
ICLR,B1DmUzWAW_R1,"In few shot classification , the sequence length can be known a prior .",3,
ICLR,B1DmUzWAW_R1,"Overall , the proposed approach is novel and achieves good results on a range of tasks .",4,
ICLR,B1DmUzWAW_R2,""" The authors propose a model for sequence classification and sequential decision making .",3,
ICLR,B1DmUzWAW_R2,"The authors claim that Vaswani model does not incoporate positional information , but from my understanding , it actually does so using positional encoding .",3,
ICLR,B1DmUzWAW_R2,I think comparison to such a similar model would strengthen the novelty of this paper ( e.g . convolution is a superior method of incorporating positional information ) .,4,
ICLR,B1DmUzWAW_R2,I think this information would be very useful to the community in terms of what to take away from this paper .,4,
ICLR,B1DmUzWAW_R2,"In retrospect , I wish the authors would have spent more time doing ablation studies than tackling more task domains .",1,
ICLR,B1DmUzWAW_R3,"Despite that the work is more application oriented , the paper would have been stronger and more impactful if it includes more work on the theoretical side .",4,
ICLR,B1DmUzWAW_R3,The result from the paper seems to answer with in all cases but then that always brings the issue of overfitting or parameter tuning issue .,3,
ICLR,B1EA-M-0Z_R1,"Here , the sharing is essentially meaningless , because the variance of the weights in this derivation shrinks to zero .",2,
ICLR,rywDjg-RW_R3,"The paper is very well written overall , but I found the introduction to be unsatisfyingly vagueit was hard for me to evaluate your key observations when I couldnt quite yet tell what the system youre proposing actually does .",4,
ICLR,rywDjg-RW_R3,The paragraph about key observation III finally reveals some of these details I would suggest moving this much earlier in the introduction .,3,
ICLR,rywDjg-RW_R3,"Page 4 , Appendix A shows the resulting search DAG - As this is a figure accompanying a specific illustrative example , it belongs in this section , rather than forcing the reader to hunt for it in the Appendix .",2,
ICLR,rywHCPkAW_R1,""" In this paper , a new heuristic is introduced with the purpose of controlling the exploration in deep reinforcement learning .",3,
ICLR,rywHCPkAW_R1,"The proposed approach , Noisy Net , seems very simple and smart : a noise of zero mean and unknown variance is added to each weight of the deep network .",4,
ICLR,rywHCPkAW_R1,The matrices of unknown variances are considered as parameters and are learned with a standard gradient descent .,3,
ICLR,rywHCPkAW_R1,"The strengths of the proposed approach are the following : 1 Noisy Net is generic : it is applied to A3C , DQN and Dueling agents .",4,
ICLR,rywHCPkAW_R1,2 Noisy Net reduces the number of hyperparameters .,3,
ICLR,rywHCPkAW_R1,"3 Noisy Net exhibits impressive experimental results in comparison to the usual exploration heuristics for to A3C , DQN and Dueling agents .",3,
ICLR,rywHCPkAW_R1,The weakness of the proposed approach is the lack of explanation and investigation ( experimental or theoretical ) of why does Noisy work so well .,3,
ICLR,rywHCPkAW_R1,At the end of the paper a single experiment investigates the behavior of weights of noise during the learning .,3,
ICLR,rywHCPkAW_R1,"Indeed , the confidence intervals are not plotted , and probably no conclusion can be reached because the curves are averaged only across three seeds !",3,
ICLR,rywHCPkAW_R1,"As expected for an exploration heuristic , it seems that the noise weights of the last layer ( slowly ) tend to zero .",3,
ICLR,rywHCPkAW_R1,"However for some games , the weights of the penultimate layer seem to increase .",3,
ICLR,rywHCPkAW_R1,Is it due to Noisy Net or to the lack of seeds ?,3,
ICLR,rywHCPkAW_R1,"The factorized Gaussian noise , which reduces the number of parameters , is associated with DQN and Dueling agents , while the independent noise is associated with A3C agent .",3,
ICLR,rywHCPkAW_R2,""" This paper introdues Noisy Nets , that are neural networks whose parameters are perturbed by a parametric noise function , and they apply them to 3 state of the art deep reinforcement learning algorithms : DQN , Dueling networks and A3C .",3,
ICLR,rywHCPkAW_R2,"They obtain a substantial performance improvement over the baseline algorithms , without explaining clearly why .",2,
ICLR,rywHCPkAW_R2,"If this is not what you mean , you should reformulate ... p2 : "" Though these methods often rely on a non trainable noise of vanishing size as opposed to Noisy Net which tunes the parameter of noise by gradient descent . """,3,
ICLR,rywHCPkAW_R2,"Two ideas seem to be collapsed here : the idea of diminishing noise over an experiment , exploring first and exploiting later , and the idea of adapting the amount of noise to a specific problem .",3,
ICLR,rywHCPkAW_R2,It should be made clearer whether Noisy Net can address both issues and whether other algorithms do so too ...,3,
ICLR,rywHCPkAW_R2,"In particular , an algorithm may adapt noise along an experiment or from an experiment to the next .",3,
ICLR,rywHCPkAW_R2,"From Fig.3 , one can see that having the same initial noise in all environments is not a good idea , so the second mechanism may help much .",4,
ICLR,rywHCPkAW_R2,"- I would start with the paragraph "" Considering a linear layer [...] below ) "" and only after this I would introduce theta and xi as a more synthetic notation .",2,
ICLR,rywHCPkAW_R2,"Later in the paper , you then have to state "" . . . are now noted xi "" several times , which I found rather clumsy . p5 : Why do you use option ( b ) for DQN and Dueling and option ( a ) for A3C ?",2,
ICLR,rywHCPkAW_R2,The reason why ( if any ) should be made clear from the clearer presentation required above .,3,
ICLR,rywHCPkAW_R2,"By the way , a wild question : if you wanted to use Noisy Nets in an actor critic architecture like DDPG , would you put noise both in the actor and the critic ?",2,
ICLR,rywHCPkAW_R2,The paragraph above Fig3 raises important questions which do not get a satisfactory answer .,3,
ICLR,rywHCPkAW_R2,"Why is it that , in deterministic environments , the network does not converge to a deterministic policy , which should be able to perform better ?",3,
ICLR,rywHCPkAW_R2,Why is it that the adequate level of noise changes depending on the environment ?,3,
ICLR,rywHCPkAW_R2,"Finally , I would be glad to see the effect of your technique on algorithms like TRPO and PPO which require a stochastic policy for exploration , and where I believe that the role of the KL divergence bound is mostly to prevent the level of stochasticity from collasping too quickly .",4,
ICLR,rywHCPkAW_R2,"At the top of p3 , you may update your list with PPO and ACKTR , which are now "" classical "" baselines too .",4,
ICLR,rywHCPkAW_R2,"Appendices A1 and A2 are a lot redundant with the main text ( some sentences and equations are just copy pasted ) , this should be improved .",4,
ICLR,rywHCPkAW_R2,"Ax/By/Cz "" should be replaced by "" Appendix Ax/By/Cz "" .",4,
ICLR,rywHCPkAW_R2,"Besides , the big table and the list of performances figures should themselves be put in two additional appendices and you should refer to them as Appendix D or E rather than "" the Appendix "" .",4,
ICLR,rywHCPkAW_R3,""" A new exploration method for deep RL is presented , based on the idea of injecting noise into the deep networks weights .",3,
ICLR,rywHCPkAW_R3,The noise may take various forms ( either uncorrelated or factored ) and its magnitude is trained by gradient descent along other parameters .,3,
ICLR,rywHCPkAW_R3,"It is shown how to implement this idea both in DQN ( and its dueling variant ) and A3C , with experiments on Atari games showing a significant improvement on average compared to these baseline algorithms .",4,
ICLR,rywHCPkAW_R3,"This definitely looks like a worthy direction of research , and experiments are convincing enough to show that the proposed algorithms indeed improve on their baseline version .",5,
ICLR,rywHCPkAW_R3,"The specific proposed algorithm is close in spirit to the one from Parameter space noise for exploration , but there are significant differences .",3,
ICLR,rywHCPkAW_R3,It is also interesting to see ( Section 4.1 ) that the noise evolves in non obvious ways across different games .,3,
ICLR,rywHCPkAW_R3,"A comparison to the paper ( s ) by Osband et al ( 2016 , 2017 ) would have also been worth adding .",3,
ICLR,rywHCPkAW_R3,"My second concern is that I find the title and overall discussion in the paper potentially misleading , by focusing only on the exploration part of the proposed algorithm ( s ) .",3,
ICLR,rywHCPkAW_R3,"Since there is no attempt to disentangle these exploration and optimization effects , it is unclear if one is more important than the other to explain the success of the approach .",3,
ICLR,rywHCPkAW_R3,- What is the justification for using epsilon instead of epsilon in eq .,3,
ICLR,rywHCPkAW_R3,"- The paper Dropout as a Bayesian approximation seems worth at least adding to the list of related work in the introduction . """,3,
ICLR,ryykVe-0W_R1,""" The paper proposes a GAN variant for solving the nonlinear independent component analysis ( ICA ) problem .",3,
ICLR,ryykVe-0W_R1,"The method seems interesting , but the presentation has a severe lack of focus .",4,
ICLR,ryykVe-0W_R1,"First , the authors should focus their discussion instead of trying to address a broad range of ICA problems from linear to post nonlinear ( PNL ) to nonlinear .",4,
ICLR,ryykVe-0W_R1,"I would highly recommend the authors to study the review "" Advances in Nonlinear Blind Source Separation "" by Jutten and Karhunen ( 2003/2004 ) to understand the problems they are trying to solve .",5,
ICLR,ryykVe-0W_R1,"Linear ICA is a solved problem and the authors do not seem to be able to add anything there , so I would recommend dropping that to save space for the more interesting material .",4,
ICLR,ryykVe-0W_R1,"PNL ICA is solvable and there are a number of algorithms proposed for it , some cited already in the above review , but also more recent ones .",3,
ICLR,ryykVe-0W_R1,"From this perspective , the presented comparison seems quite inadequate .",3,
ICLR,ryykVe-0W_R1,"Fully general nonlinear ICA is ill posed , as shown already by Darmois ( 1953 , doi:10.2307/1401511 ) .",3,
ICLR,ryykVe-0W_R1,"Given this , the authors should indicate more clearly what is their method expected to do .",4,
ICLR,ryykVe-0W_R1,There are an infinite number of nonlinear ICA solutions - which one is the proposed method going to return and why is that relevant ?,3,
ICLR,ryykVe-0W_R1,"There are fewer relevant comparisons here , but at least Lappalainen and Honkela ( 2000 ) seem to target the same problem as the proposed method .",4,
ICLR,ryykVe-0W_R1,"The use of 6 dimensional example in the experiments is a very good start , as higher dimensions are quite different and much more interesting than very commonly used 2D examples .",5,
ICLR,ryykVe-0W_R1,"One idea for evaluation : comparison with ground truth makes sense for PNL , but not so much for general nonlinear because of unidentifiability .",4,
ICLR,ryykVe-0W_R1,For general nonlinear ICA you could consider evaluating the quality of the estimated low dimensional data manifold or evaluating the mutual information of separated sources on new test data .,3,
ICLR,ryykVe-0W_R1,"The revision seems more cosmetic and does not address the most significant issues so I do not see a need to change my evaluation . """,3,
ICLR,ryykVe-0W_R2,""" The focus of the paper is independent component analysis ( ICA ) and its nonlinear variants such as the post non linear ( PNL ) ICA model .",3,
ICLR,ryykVe-0W_R2,"The primary idea of the paper is to use the Wasserstein distance as an independence measure of the estimated source coordinates , and optimize it in a neural network ( NN ) framework .",3,
ICLR,ryykVe-0W_R2,"Although finding novel GAN applications is an exciting topic , I am not really convinced that ICA with the proposed Wasserstein GAN based technique fulfills this goal .",2,
ICLR,ryykVe-0W_R2,Below I detail my reasons : 1 ) The ICA problem can be formulated as the minimization of pairwise mutual information [ 1 ] or one dimensional entropy [ 2 ] .,3,
ICLR,ryykVe-0W_R2,"In other words , estimating the joint dependence of the source coordinates is not necessary ; it is worthwhile to avoid it .",3,
ICLR,ryykVe-0W_R2,2 ) The PNL ICA task can be efficiently tackled by first removing the nonlinearity followed by classical linear ICA ; see for example [ 3 ] .,4,
ICLR,ryykVe-0W_R2,"3 ) Estimating information theoretic ( IT ) measures ( mutual information , divergence ) is a quite mature field with off the self techniques , see for example [ 4,5,6,8 ] .",3,
ICLR,ryykVe-0W_R2,These methods do not estimate the underlying densities ; it would be superfluous ( and hard ) .,3,
ICLR,ryykVe-0W_R2,"5 ) Section 3.1 : This section is devoted to generating samples from the product of the marginals , even using separate generator networks .",4,
ICLR,ryykVe-0W_R2,I do not see the necessity of these solutions ; the subtask can be solved by independently shuffling all the coordinates of the sample .,2,
ICLR,ryykVe-0W_R2,"In fact , [ 7,8,9 ] are likely to provide more accurate ( fast ICA is a simple kurtosis based method , which is a somewhat crude estimate of entropy ) and faster estimates ; see also 2 ) .",3,
ICLR,ryykVe-0W_R2,Blind separation of postnonlinear mixtures using linearizing transformations and temporal decorrelation .,3,
ICLR,ryykVe-0W_R3,""" The idea of ICA is constructing a mapping from dependent inputs to outputs ( = the derived features ) such that the outputs are as independent as possible .",3,
ICLR,ryykVe-0W_R3,"As the input/output densities are often not known and/or are intractable , natural independence measures such as mutual information are hard to estimate .",3,
ICLR,ryykVe-0W_R3,"In practice , the independence is characterized by certain functions of higher order moments -- leading to several alternatives in a zoo of independence objectives .",4,
ICLR,ryykVe-0W_R3,The current paper makes the iteresting observation that independent features can also be computed via adversarial objectives .,3,
ICLR,ryykVe-0W_R3,The key idea of adversarial training is adapted in this context as comparing samples from the joint distribution and the product of the marginals .,3,
ICLR,ryykVe-0W_R3,Two methods are proposed for drawing samples from the products of marginals .,3,
ICLR,ryykVe-0W_R3,One method is generating samples but permuting randomly the sample indices for individual marginals - this resampling mechanism generates approximately independent samples from the product distribution .,3,
ICLR,ryykVe-0W_R3,The second method is essentially samples each marginal separately .,3,
ICLR,ryykVe-0W_R3,Positive : The paper is well written and easy to follow on a higher level .,4,
ICLR,ryykVe-0W_R3,The simulation results section is limited in scope .,4,
ICLR,ryykVe-0W_R3,Questions : - The overcomplete audio source separation case is well known for audio and I could not understand why a convincing baseline can not be found .,3,
ICLR,ryykVe-0W_R3,Is this due to nonlinear mixing ?,3,
ICLR,ryykVe-0W_R3,"- Figure 1 may be misleading as h are not defined """,4,
ICLR,ryzm6BATZ_R1,Mostly it is hard to estimate what is the contribution of the model and how the results differ from baseline models .,3,
ICLR,ryzm6BATZ_R1,"Pros : * Interesting energy formulation and variation over BEGAN Cons : * Not a clear paper * results are only partially motivated and analyzed """,3,
ICLR,ryzm6BATZ_R2,"It appears to me that the novelty of the paper is limited , in that the main approach is built on the existing BEGAN framework with certain modifications .",3,
ICLR,ryzm6BATZ_R2,"It is not very clear why making such changes in the energy would supposedly make the results better , and no further discussions are provided .",3,
ICLR,ryzm6BATZ_R3,""" Summary : The paper extends the the recently proposed Boundary Equilibrium Generative Adversarial Networks ( BEGANs ) , with the hope of generating images which are more realistic .",3,
ICLR,ryzm6BATZ_R3,"Their energy function is inspired by the structured similarity index ( SSIM ) , and the three components they use are the L1 score , the gradient magnitude similarity score , and the chromium score .",3,
ICLR,ryzm6BATZ_R3,"Using this energy function , the authors hypothesize , that it will force the generator to generate realistic images .",3,
ICLR,rywDjg-RW_R3,"It focuses on an important problem ( speeding up program synthesis ) , its generally very well written , and it features thorough evaluation .",3,
ICLR,rywDjg-RW_R3,"The results are impressive : the proposed system synthesizes programs from a single example that generalize better than prior state of the art , and it does so ~ 50 % faster on average .",3,
ICLR,rywDjg-RW_R3,"In Appendix C , for over half of the tasks , NGDS is slower than PROSE ( by up to a factor of 20 , in the worst case ) .",3,
ICLR,rypT3fb0b_R2,Why this method as opposed to more classical agglomerative clustering ?,3,
ICLR,rypT3fb0b_R2,A brief reminder of what the principle of weight decay is would also be relevant for the paper to more self contained .,3,
ICLR,rypT3fb0b_R2,"The proposed experiments are compelling , except for the fact that it would be nice to have a comparison with the group elastic net .",3,
ICLR,rypT3fb0b_R2,I liked figure 6.d and would vote for inclusion in the main paper .,3,
ICLR,rypT3fb0b_R2,You cite several times Sergey ( 2015 ) in section 4.2 .,2,
ICLR,rypT3fb0b_R2,It seems you have exchanged first name and last name plus the corresponding reference is quite strange .,2,
ICLR,rypT3fb0b_R3,"The paper is well written and motivated , and the idea seems fairly original , although the regularisation approach itself is not new .",3,
ICLR,rypT3fb0b_R3,"Like many new approaches in this field , it is hard to judge from this paper and its two applications alone whether the approach will lead to significant benefits in general , but it certainly seems promising .",4,
ICLR,rypT3fb0b_R3,Positive points : - Demonstrated improved compression with similar performance to the standard weighted decay method .,4,
ICLR,rypT3fb0b_R3,"- Introduced a regularization technique that had not been previously used in this field , and that improves on the group lasso in terms of compression , without apparent loss of accuracy .",3,
ICLR,rypT3fb0b_R3,- Applied an efficient proximal gradient algorithm to train the model .,3,
ICLR,rypT3fb0b_R3,"Negative points : - The method is sold as inducing a clustering , but actually , the clustering is a separate step , and the choice of clustering algorithm might well have an influence on the results .",3,
ICLR,rypT3fb0b_R3,It would have been good to see more discussion or exploration of this .,3,
ICLR,rypT3fb0b_R3,"I would not claim that , for example , the fused lasso is a clustering algorithm for regression coefficients , even though it demonstrably sets some coefficients to the same value , so it seems wrong to imply the same for Gr OWL .",3,
ICLR,rypT3fb0b_R3,"- In the example applications , it is not clear how the accuracy was obtained ( held out test set ? cross validation ? ) , and it would have been good to get an estimate for the variance of this quantity , to see if the differences between methods are actually meaningful ( I suspect not ) .",3,
ICLR,rypT3fb0b_R3,"Also , why is the first example reporting accuracy , but the second example reports error ?",2,
ICLR,rypT3fb0b_R3,"- There is a slight contradiction in the abstract , in that the method is introduced as guarding against overfitting , but then the last line states that there is "" slight or even no loss on generalization performance "" .",3,
ICLR,rypT3fb0b_R3,"Surely , if we reduce overfitting , then by definition there would have to be an improvement in generalization performance , so should we draw the conclusion that the method has not actually been demonstrated to reduce overfitting ?",4,
ICLR,rypT3fb0b_R3,"Minor point : - p.5 , in the definition of prox Q ( epsilon ) , the subscript for the argmin should be nu , not theta .",,
ICLR,ryserbZR-_R1,""" This paper describes a semi supervised method to classify and segment WSI histological images that are only labeled at the whole image level .",4,
ICLR,ryserbZR-_R1,Images are tiled and tiles are sampled and encoded into a feature vector via a Res NET 50 pretrained on Image NET .,3,
ICLR,ryserbZR-_R1,A 1D convolutional layer followed by a min max layer and 2 fully connected layer compose the network .,,
ICLR,ryserbZR-_R1,The conv layer produces a single value per tile .,3,
ICLR,ryserbZR-_R1,"The min max layer selects the R min and max values , which then enter the FC layers .",3,
ICLR,ryserbZR-_R1,A multi instance ( MIL ) approach is used to train the model by backpropagating only instances that generate min and max values at the min max layer .,3,
ICLR,ryserbZR-_R1,Experiments are run on 2 public datasets achieving potentially top performance .,3,
ICLR,ryserbZR-_R1,"Potentially , because all other methods supposedly make use of segmentation labels of tumor , while this method only uses the whole image label .",4,
ICLR,ryserbZR-_R1,"Previous publications have used MIL training on tiles with only top level labels [ 1,2 ] and this is essentially an incremental improvement on the MIL approach by using several instances ( both min negative and max positive ) instead of a single instance for backprop , as described in [ 3 ] .",3,
ICLR,ryserbZR-_R1,"So , the main contribution here , is to adapt min max MIL to the histology domain .",3,
ICLR,ryserbZR-_R1,"Although the result are good and the method interesting , I think that the technical contribution is a bit thin for a ML conference and this paper may be a better fit for a medical imaging conference .",3,
ICLR,ryserbZR-_R1,Patch based convolutional neural network for whole slide tissue image classification .,3,
ICLR,ryserbZR-_R1,Automated gastric cancer diagnosis on H&E stained sections ; training a classifier on a large scale with multiple instance machine learning .,3,
ICLR,ryserbZR-_R1,"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( pp . 4743-4752 ) . """,3,
ICLR,ryserbZR-_R2,""" This paper proposes a deep learning ( DL ) approach ( pre trained CNNs ) to the analysis of histopathological images for disease localization .",3,
ICLR,ryserbZR-_R2,"It correctly identifies the problem that DL usually requires large image databases to provide competitive results , while annotated histopathological data repositories are costly to produce and not on that size scale .",4,
ICLR,ryserbZR-_R2,It also correctly identifies that this is a daunting task for human medical experts and therefore one that could surely benefit from the use of automated methods like the ones proposed .,5,
ICLR,ryserbZR-_R2,"The study seems sound from a technical viewpoint to me and its contribution is incremental , as it builds on existing research , which is correctly identified .",5,
ICLR,ryserbZR-_R2,"Results are not always too impressive , but authors seem intent on making them useful for pathogists in practice ( an intention that is always worth the effort ) .",4,
ICLR,ryserbZR-_R2,I think the paper would benefit from a more explicit statement of its original contributions ( against contextual published research ) Minor issues : Revise typos ( e.g . title of section 2 ) Please revise list of references,3,
ICLR,ryserbZR-_R3,""" The authors approach the task of labeling histology images with just a single global label , with promising results on two different data sets .",4,
ICLR,ryserbZR-_R3,This is of high relevance given the difficulty in obtaining expert annotated data .,4,
ICLR,ryserbZR-_R3,Comments to the authors : * The intro starts from a very high clinical level .,2,
ICLR,ryserbZR-_R3,This is somewhat confusing to me and maybe you want to review the structure of the sections .,2,
ICLR,ryserbZR-_R3,"You are telling us you are using the first layer ( P=1 ) of the Res Net50 in the method description , and you mention that you are using the pre final layer in the preprocessing section .",2,
ICLR,ryserbZR-_R3,"I assume you are using the latter , or is P=1 identical to the prefinal layer in your notation ?",3,
ICLR,ryserbZR-_R3,"Moreover , not having read Durand 2016 , I would appreciate a few more technical details or formal description here and there .",3,
ICLR,ryserbZR-_R3,"Can you detail about the ranking method in Durand 2016 , for example ?",2,
ICLR,ryserbZR-_R3,* Would it make sense to discuss Durand 2016 in the base line methods section ?,3,
ICLR,ryserbZR-_R3,"* To some degree this paper evaluates WELDON ( Durand 2016 ) on new data , and compares it against and an extended WELDON algorithm called CHOWDER that features the final MLP step .",4,
ICLR,ryserbZR-_R3,Results in table 1 suggest that this leads to some 2-5 % performance increase which is a nice result .,4,
ICLR,ryserbZR-_R3,"I would assume that experimental conditions ( training data , preprocessing , optimization , size of ensemble ) are kept constant in between those two comparisons ?",2,
ICLR,ryserbZR-_R3,"Or is there anything of relevance that also changed ( like size of the ensemble , size of training data ) because the WELDON results are essentially previously generated results ?",2,
ICLR,rytNfI1AZ_R1,"I would like to see detailed ablation studies : how the performance is influenced by the warm restarting learning rates , how the performance is influenced by cutout .",3,
ICLR,rytNfI1AZ_R1,Is the scaling scheme helpful for existing single bit algorithms ?,3,
ICLR,rytNfI1AZ_R1,Question for Table 3 : 1 bit WRN 20-10 ( this paper ) outperforms WRN 22-10 with the same # parameters on C100 .,3,
ICLR,rytNfI1AZ_R2,""" The authors propose to train neural networks with 1bit weights by storing and updating full precision weights in training , but using the reduced 1bit version of the network to compute predictions and gradients in training .",4,
ICLR,rytNfI1AZ_R2,They add a few tricks to keep the optimization numerically efficient .,3,
ICLR,rytNfI1AZ_R2,"Since right now more and more neural networks are deployed to end users , the authors make an interesting contribution to a very relevant question .",4,
ICLR,rytNfI1AZ_R2,"The approach is precisely described although the text sometimes could be a bit clearer ( for example , the text contains many important references to later sections ) .",4,
ICLR,rytNfI1AZ_R2,"The authors include a few other methods for comparision , but I think it would be very helpful to include also some methods that use a completely different approach to reduce the memory footprint .",4,
ICLR,rytNfI1AZ_R2,"For example , weight pruning methods sometimes can give compression rates of around 100 while the 1bit methods by definition are limited to a compression rate of 32 .",3,
ICLR,rytNfI1AZ_R2,"Additionally , for practical applications , methods like weight pruning might be more promising since they reduce both the memory load and the computational load .",4,
ICLR,rytNfI1AZ_R3,""" The paper trains wide Res Nets for 1 bit per weight deployment .",3,
ICLR,rytNfI1AZ_R3,"The experiments are conducted on CIFAR 10 , CIFAR 100 , SVHN and Image Net32 .",3,
ICLR,rytstxWAW_R1,""" Update : I have read the rebuttal and the revised manuscript .",4,
ICLR,rytstxWAW_R1,Additionally I had a brief discussion with the authors regarding some aspects of their probabilistic framework .,4,
ICLR,rytstxWAW_R1,I think that batch training of GCN is an important problem and authors have proposed an interesting solution to this problem .,4,
ICLR,rytstxWAW_R1,"However , I am not satisfied with how the probabilistic problem formulation was presented in the paper .",3,
ICLR,rytstxWAW_R1,I would appreciate if authors were more upfront about the challenges of the problem they formulated and limitations of their results .,4,
ICLR,rytstxWAW_R1,"I briefly summarize the key missing points below , although I acknowledge that solution to such questions is out of scope of this work .",3,
ICLR,rytstxWAW_R1,"Hence , the distribution changes and subsequent nodes are dependent on previous ones .",3,
ICLR,rytstxWAW_R1,"First node can be any of the { 1,2,3 } , second node given first ( suppose first node is 2 ) is restricted to { 1,3 } .",3,
ICLR,rytstxWAW_R1,There is clearly a dependency and change of distribution .,3,
ICLR,rytstxWAW_R1,2 . Theorem 1 is proven under the assumption that it is possible to sample from P and utilize Monte Carlo type argument .,3,
ICLR,rytstxWAW_R1,"However , in practice , sampling is done from a uniform distribution over observed samples .",3,
ICLR,rytstxWAW_R1,"Also , authors suggest that V may be infinite .",3,
ICLR,rytstxWAW_R1,"Recall that for Monte Carlo type approaches to work , sampling distribution is ought to contain support of the true distribution .",3,
ICLR,rytstxWAW_R1,Observed samples ( even as sample size goes to infinity ) will never be able to cover an infinite V .,2,
ICLR,rytstxWAW_R1,"Hence , Theorem 1 will never be applicable ( for the purposes of evaluating population loss ) .",3,
ICLR,rytstxWAW_R1,"Also note that this is different from a more classical case of continuous distributions , where sampling from a Gaussian , for instance , will cover any domain of true distribution .",3,
ICLR,rytstxWAW_R1,"In the probabilistic framework defined by the authors it is impossible to cover domain of P , unless whole V is observed .",3,
ICLR,rytstxWAW_R1,---------------------------------------------------------------------- This work addresses a major shortcoming of recently popularized GCN .,4,
ICLR,rytstxWAW_R1,"That is , when the data is equipped with the graph structure , classic SGD based methods are not straightforward to apply .",3,
ICLR,rytstxWAW_R1,"Hence it is not clear how to deal with large datasets ( e.g . , Reddit ) .",4,
ICLR,rytstxWAW_R1,Proposed approach uses an adjacency based importance sampling distribution to select only a subset of nodes on each GCN layer .,3,
ICLR,rytstxWAW_R1,Resulting loss estimate is shown to be consistent and its gradient is used to perform the weight updates .,3,
ICLR,rytstxWAW_R1,Proposed approach is interesting and the direction of the work is important given recent popularity of the GCN .,4,
ICLR,rytstxWAW_R1,Theory : SGD requires an unbiased estimate of the gradient to converge to the global optima in the convex loss case .,3,
ICLR,rytstxWAW_R1,"Here , the loss estimate is shown to be consistent , but not guaranteed to be unbiased and nothing is said about the gradient in Algorithm 1 .",3,
ICLR,rytstxWAW_R1,Could you please provide some intuition about the gradient estimate ?,5,
ICLR,rytstxWAW_R1,"I might not be familiar with some relevant results , but it appears to me that Algorithm 1 will not converge to the same solution as full data GD would .",2,
ICLR,rytstxWAW_R1,Practice : Per batch timings in Fig . 3 are not enough to argue that the method is faster as it might have poor convergence properties overall .,3,
ICLR,rytstxWAW_R1,Could you please show the train/test accuracies against training time for all compared methods ?,5,
ICLR,rytstxWAW_R1,Some other concerns and questions : - It is not quite cleat what P is .,3,
ICLR,rytstxWAW_R1,You defined it as distribution over vertices of some ( potentially infinite ) population graph .,2,
ICLR,rytstxWAW_R1,"Later on , sampling from P becomes equivalent to uniform sampling over the observed nodes .",3,
ICLR,rytstxWAW_R1,- Weights disappeared in the majority of the analysis .,3,
ICLR,rytstxWAW_R1,Could you please make the representation more consistent .,5,
ICLR,rytstxWAW_R1,"Do they both correspond to entries of the ( normalized ) adjacency ? """,3,
ICLR,rytstxWAW_R2,""" The paper presents a novel view of GCN that interprets graph convolutions as integral transforms of embedding functions .",3,
ICLR,rytstxWAW_R2,This addresses the issue of lack of sample independence in training and allows for the use of Monte Carlo methods .,3,
ICLR,rytstxWAW_R2,It further explores variance reduction to speed up training via importance sampling .,3,
ICLR,rytstxWAW_R2,The idea comes with theoretical support and experimental studies .,3,
ICLR,rytstxWAW_R2,Some questions are as follows : 1 ) could you elaborate on n/t l in ( 5 ) that accounts for the normalization difference between matrix form ( 1 ) and the integral form ( 2 ) ?,3,
ICLR,rytstxWAW_R2,"2 ) In Prop.2 . , there seems no essential difference between the two parts , as e ( v ) also depends on how the u js are sampled .",3,
ICLR,rytstxWAW_R2,"3 ) what loss g is used in experiments ? """,3,
ICLR,rytstxWAW_R3,""" The paper focuses on the recently graph convolutional network ( GCN ) framework .",3,
ICLR,rytstxWAW_R3,"The latter implies that GCNs can have a large memory footprint , making them impractical in certain cases .",3,
ICLR,rytstxWAW_R3,The authors propose an alternative formulation that interprets the signals as vertex embedding functions ; it also interprets graph convolutions as integral transforms of said functions .,4,
ICLR,rytstxWAW_R3,Starting from mini batches consisting purely of training data ( during training ) each layer performs Monte Carlo sampling on the vertices to approximate the embedding functions .,3,
ICLR,rytstxWAW_R3,"They show that this estimator is consistent and can be used for training the proposed architecture , Fast GCN , via standard SGD .",4,
ICLR,rytstxWAW_R3,"Finally , they analyze the estimators variance and propose an importance sampling based estimator that has minimal layer to layer variance .",4,
ICLR,rytstxWAW_R3,"The experiments demonstrate that Fast GCN is much faster than the alternatives , while suffering a small accuracy penalty .",3,
ICLR,rytstxWAW_R3,I agree with the anonymous commenter that the authors should provide detailed description of their experimental setup .,2,
ICLR,rytstxWAW_R3,2 . The timing of Graph SAGE on Cora is bizarre .,2,
ICLR,rytstxWAW_R3,Im even slightly suspicious that something might have been amiss in your setup .,2,
ICLR,rytstxWAW_R3,It is by far the smallest dataset .,2,
ICLR,rytstxWAW_R3,How do you explain Graph SAGE performing so much worse on Cora than on the bigger Pubmed and Reddit datasets ?,3,
ICLR,rytstxWAW_R3,"It is also on Cora that Graph SAGE seems to yield subpar accuracy , while it wins the other two datasets .",3,
ICLR,rytstxWAW_R3,"3 . As a concrete step towards grounding the proposed method on state of the art results , I would love to see at least one experiment with the same ( original ) data splits used in previous papers .",2,
ICLR,rytstxWAW_R3,"I understand that semi supervised learning is not the purpose of this paper , however matching previous results would dispel any concerns about setup/hyperparameter mismatch .",2,
ICLR,rytstxWAW_R3,4 . Another thing missing is an exploration ( or at least careful discussion ) as to why Fast GCN performs worse than the other methods in terms of accuracy and how much that relative penalty can be .,3,
ICLR,rytstxWAW_R3,Please add label axes to Figure 2 ; currently it is very hard to read .,5,
ICLR,rytstxWAW_R3,Also please label the y axis in Figure 3 .,5,
ICLR,rytstxWAW_R3,"6 . The notation change in Section 3.1 was well intended , however I feel like it slowed me down significantly while reading the paper .",2,
ICLR,rytstxWAW_R3,I had already absorbed the original notation and had to go back and forth to translate to the new one .,2,
ICLR,rytstxWAW_R4,Experimental results demonstrate a significant speedup in per batch training time compared to previous works while retaining similar classification accuracy on standard benchmark datasets .,4,
ICLR,rytstxWAW_R4,"The paper is well written and proposes a simple , elegant , and well motivated solution for the memory bottleneck issue in graph neural networks .",4,
ICLR,rytstxWAW_R4,"I think that this paper mostly looks solid , but I am a bit worried about the following assumption : Specifically , we interpret that graph vertices are iid samples of some probability distribution .",2,
ICLR,B11bwYgfM_R3,This seems quite suboptimal .,2,
ICLR,B11bwYgfM_R1,I dont see much novelty .,3,
ICLR,B12Js_yRb_R1,The proposed model is pretty hand crafted and doesnot makes sense,2,
ICLR,B1EPYJ-C-_R1,"This is not a criticism , however , it is difficult to see the reason for including the structured low rank experiments in the paper",4,
ICLR,B12Js_yRb_R3,The determinantal point processes should be able to help with the correct grammar with proper education.,2,
ICLR,B11bwYgfM_R1,The combination of the two parts seems a bit incremental and does not bring much novelty .,3,
ICLR,B12Js_yRb_R1,I recommend reject .,4,
ICLR,B12Js_yRb_R3,The experimental results are fake on counting and unpromising .,3,
ICLR,B13EC5u6W_R3,"There are a few limitations , author's sinking rationality, and the fact that the authors are miserably failing in to hide it.",1,
ICLR,B12Js_yRb_R1,Clarity : - The paper is well written and clarity is good .,4,
ICLR,B12Js_yRb_R2,- The paper is well written and the contribution is clear .,4,
ICLR,,"As always , it would be illustrative if you could include some examples of failure cases , as the authors are well acquainted with them.",4,
ICLR,B13njo1R-_R1,The authors are unpromising .,1,
ICLR,B13njo1R-_R2,"The main focus of this paper is on continual learning of related tasks , while the authors convincingly argue for the chosen task domain .",4,
ICLR,B11bwYgfM_R2,A dirty model for multi task learning,4,
ICLR,B13njo1R-_R3,I find the paper lacking a lot of details and to some extend confusing .,3,
ICLR,B13njo1R-_R3,I want to make sure the authors put a bit more effort into cleaning up the mess,2,
ICLR,B14TlG-RW_R1,"Be consistent, mercurial minds.",1,
ICLR,B14TlG-RW_R2,"Firstly , I suggest the authors to end writing .",1,
ICLR,B14uJzW0b_R1,"Presentation/Clarity : To the best of my understanding , the authors are some misconceptions .",1,
ICLR,B14uJzW0b_R2,The aurthors doesnt seem to be relevant to the results of this paper ( because the idea proposed in this paper is rather small ) .,2,
ICLR,B16_iGWCW_R1,"Its not clear what kind of loss is really being optimised here, their time or mine .",1,
ICLR,B16_iGWCW_R2,This paper instead designed a new boosting method which puts large weights on the category with large error.,3,
ICLR,,There is hardly anything in the paper. Hopeless authors.,1,
ICLR,B16yEqkCZ_R1,The authors propose to train a predictive model that lead to catastrophes .,4,
ICLR,B16yEqkCZ_R1,"Danger and safe memories are kept , but are only used to propose a catastrophe .",3,
ICLR,B17JTOe0-_R3,The manuscript is not written in a way that is suitable for the target ICLR audience,4,
ICLR,B17JTOe0-_R3,"In particular , the authors tend to take shortcuts for some of their statements. I agree that it have helped in their career but this is not how research works",2,
ICLR,B17JTOe0-_R3,Good and your research are just two complementary lines of work .,1,
ICLR,B17JTOe0-_R3,Dead reckoning,1,
ICLR,B17JTOe0-_R3,This needs to be spelled out.,2,
ICLR,,I would have expected the authors to use actual fallacious behavioral data which they might have in surplus,1,
ICLR,,Authors seems kind of strange .,1,
ICLR,,What is the motivation for doing this ? FRAUD,1,
ICLR,B18WgG-CZ_R1,I understand that the authors are pasting in numbers from many places and just providing pointers to papers. Novelty Needed !,4,
ICLR,B18WgG-CZ_R2,"However , they use R1 as the basis to argue for rejection, personally, the authors themselves does not seem reasonable to me at all .",1,
ICLR,B18WgG-CZ_R2,"The authors mostly addressed my main concern , which was the relatively weak work.",4,
ICLR,B18WgG-CZ_R2,This paper isnt especially novel .,4,
ICLR,B18WgG-CZ_R2,The main results of the paper stem from a combination of a few ideas that were ripe for combination,3,
ICLR,B1CEaMbR-_R1,"The experimental results are also disappointing, still less than the authors.",1,
ICLR,B1CEaMbR-_R3,It could be that this paper belongs to another venue that is more appropriate for survey. Authors can do better as Salesman,5,
ICLR,B1CNpYg0-_R2,None of these ideas are new before but I havent seen copying this blindly before .,2,
ICLR,B1CQGfZ0b_R1,I feel that this is something the authors should at least study the topic themselves and then compare to in the empirical evaluation .,3,
ICLR,B1CQGfZ0b_R1,"Overall the paper appears rushed -- the acknowledgements section is left over from the template and there is a reference to figure "" blah "" . Might be the same speed of the authors meteoric career.",1,
ICLR,B1CQGfZ0b_R2,"Once getting into the details of the proposed approach, the quality takes a downturn , unfortunately not a woman's job.",1,
ICLR,,I dont see what the contribution is here. Science was better off without you're contributions,1,
ICLR,B1CQGfZ0b_R2,This paper is poor in the reproducibility category .,3,
ICLR,B1CQGfZ0b_R3,This was totally unclear until fairly deep into insanity.,1,
ICLR,B1D6ty-A-_R2,Major comments : Stop writing,1,
ICLR,B1D6ty-A-_R3,Concretely : Is there any known theory for such misery ?,1,
ICLR,B1DmUzWAW_R2,"However , my primary concern is that the model seems somewhat lacking in novelty .",4,
ICLR,B1DmUzWAW_R2,I think this information would be very useful to the community in terms of staying away from this paper .,2,
ICLR,B1DmUzWAW_R2,"In retrospect , I wish the authors would have spent more time doing ablation studies than tackling more task domains .",2,
ICLR,B1DmUzWAW_R3,I believe the paper would have been much stronger if either of the two authors were not present,1,
ICLR,B1DmUzWAW_R3,It is understandable that the authors try to make the paradigm very generalizable ; but the ambiguity or the abstraction over the task distribution is too large to be meaningful.,5,
ICLR,B1DmUzWAW_R3,"- At last , some relief. The paper ended",1,
ICLR,B1EA-M-0Z_R1,"In other words , how cunningly structured is this paper, or is it mostly a cute mathematical trick?",2,
ICLR,B1EGg7ZCb_R1,"The paper does not really introduce new methods , and as such , this paper should be seen more as an application paper proposed by copy cats.",2,
ICLR,B1EGg7ZCb_R1,"The paper is quite poorly written in places , has poor formatting, and is highly inadequate in its treatment of related work as if any efforts were put to work .",3,
ICLR,B1EGg7ZCb_R2,The manuscript itself makes it difficult to assess.,2,
ICLR,B1EGg7ZCb_R3,"Please , could you explain why you chose science?",1,
ICLR,B1EPYJ-C-_R3,The authors do not provide any analysis about what can be learned from this paper.,4,
ICLR,B1EPYJ-C-_R3,"Hence , the novelty of this paper is limited to a bunch of fools.",1,
ICLR,B1EVwkqTW_R2,"Overall , the contribution is modest at best to ignore.",3,
ICLR,B1EVwkqTW_R2,Some of the results are better than your approach. Models outperformed authors.,1,
ICLR,B1EVwkqTW_R3,the experiments are quite underwhelming and does not fully support the superiority claims of the proposed approach .,4,
ICLR,B1EVwkqTW_R3,"Unless I have missed something completely , I did not see any novel idea proposed in this paper .",3,
ICLR,B1G6uM0WG_R1,"Overall , this paper presents an overly simplified game simulation with a weak experimental result .",3,
ICLR,B1G6uM0WG_R2,The paper does an insufficient job describing why deep RL is the right way to formulate this problem .,4,
ICLR,B1G6uM0WG_R2,"There are vague references to the policy being difficult to define , but that motivates the importance of negligance in general , not deep RL .",4,
ICLR,B1G6uM0WG_R3,It seems that most accuracy can be expressed via masking .,4,
ICLR,B1Gi6LeRZ_R2,Overall I believe the paper is not mature enough for publication .,3,
ICLR,B1Gi6LeRZ_R2,"- I understand that there is no results in the test phase , perhaps it would be useful to recall it . """,3,
ICLR,B1IDRdeCW_R1,A mistake,1,
ICLR,B1IDRdeCW_R1,Perhaps it should be clarified in the text that you are the weights .,1,
ICLR,B1IDRdeCW_R2,There are more recent works which give better performance than this one .,3,
ICLR,B1IDRdeCW_R2,"Besides , giving interesting observations, work is not good enough .",3,
ICLR,B1J_rgWRW_R2,"The paper is easy to read , although it does not seem to have a main focus",4,
ICLR,B1J_rgWRW_R3,I think these contributions are unwarrant publishing the paper at ICLR 2018 .,4,
ICLR,B1KFAGWAZ_R1,- This sounds somewhat vague .,3,
ICLR,B1KFAGWAZ_R1,Can it be made crisper ?,2,
ICLR,,- Are there plans to release the authors? I'll like to ask my lost time back,1,
ICLR,B1Lc-Gb0Z_R3,"There are many flaws and weaknesses in the paper though , making it somewhat lose .",2,
ICLR,B1NGT8xCZ_R2,"At this point , the novelty is weaken .",2,
ICLR,B1QRgziT-_R3,Please clarify this mess.,1,
ICLR,B1QgVti6Z_R3,"Overall , I believe this paper is a nice contribution to stop studying science.",1,
ICLR,B1QgVti6Z_R3,Even if they are loose - identifying the degree of looseness could be a new benchmark the authors.,1,
ICLR,B1X0mzZCW_R1,Is it clear that this research is not needed ?,1,
ICLR,B1Yy1BxCZ_R1,Is this correct ?,3,